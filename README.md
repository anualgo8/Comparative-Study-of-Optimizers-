ğŸ›ï¸ E-COMMERCE TEXT CLASSIFICATION USING MULTIPLE OPTIMIZERS
ğŸ“˜ Project Overview
This project implements an E-commerce product text classification system using deep learning techniques. The model classifies product descriptions into appropriate categories such as Household, Clothing, Electronics, etc. It also compares the performance of multiple optimizers â€” including SGD, Adam, RMSProp, Adagrad, and Nadam â€” to identify which achieves the best accuracy and convergence speed.
ğŸ§­ Workflow
Each step is modular and automated â€” from dataset loading to model evaluation.
â€¢	Dataset â†’ Preprocessing â†’ Tokenization â†’ Model Training (with multiple optimizers) â†’ Evaluation â†’ Visualization
âš™ï¸ Features
â€¢	Automatic dataset download from Kaggle via kagglehub
â€¢	Tokenization and padding using TensorFlowâ€™s Tokenizer
â€¢	Multi-class label encoding for product categories
â€¢	Training and validation with five optimizers (SGD, Adam, RMSProp, Adagrad, Nadam)
â€¢	Comparative analysis of accuracy and loss curves
â€¢	Reproducibility through fixed random seeds
ğŸ“‚ Input Dataset
Source: Kaggle â€” E-commerce Text Classification Dataset
Sample Columns:
Column	Example
Category	Household
Text	SAF 'Floral' Framed Painting (Wood, 30 inch x 40 inch)...
ğŸ§  How It Works
â€¢	1ï¸âƒ£ Dataset Loading
The dataset is automatically fetched using kagglehub and loaded as a DataFrame.
â€¢	2ï¸âƒ£ File Detection
Identifies the main .csv file dynamically for loading.
â€¢	3ï¸âƒ£ Data Preparation
Extracts text and label columns, encodes labels, and splits into train and test sets.
â€¢	4ï¸âƒ£ Tokenization & Padding
Converts text to sequences and pads them to uniform length.
â€¢	5ï¸âƒ£ Model Training
Trains using different optimizers to compare convergence and validation accuracy.
â€¢	6ï¸âƒ£ Evaluation
Plots validation accuracy/loss and compares final optimizer performance.
ğŸ“Š Example Results
Optimizer	Final Validation Accuracy	Final Validation Loss
Adam	0.89	0.31
RMSProp	0.87	0.35
Nadam	0.88	0.34
Adagrad	0.72	0.68
SGD	0.65	0.75
Adam, Nadam, and RMSProp perform best, while SGD and Adagrad show slower learning and higher loss values.
ğŸš€ How to Run the Script
Prerequisites: Python 3.8+ and the following libraries:
â€¢	pip install tensorflow pandas scikit-learn kagglehub matplotlib
Run the notebook sequentially: Dataset download â†’ Preprocessing â†’ Tokenization â†’ Model training â†’ Evaluation
ğŸ“¤ Outputs
â€¢	Accuracy and loss graphs for each optimizer
â€¢	Final summary table comparing all optimizers
â€¢	Sample predictions on test data
ğŸ¤ Contributors
Anushika Gupta (Team Lead) â€” Model Design & Optimizer Analysis
Akash â€” Data Preparation 
Suryansh jain-Evaluation Scripts
Supervised by:
Dr. Rajni Ranjan Singh (HoD, CAI Department, MITS)
ğŸ“œ License
This repository is provided for academic and educational purposes only. Reproduction or redistribution without permission is prohibited.
â­ Acknowledgment
We thank Madhav Institute of Technology and Science (MITS), Gwalior for their guidance and support in completing this project.

